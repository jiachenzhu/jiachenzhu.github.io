<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformers without Normalization - DynamicTanh - DyT</title>
    <!-- Bootstrap CSS -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.2/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome for icons -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" rel="stylesheet">
    <!-- Google Fonts - Nunito as Avenir Next alternative -->
    <link href="https://fonts.googleapis.com/css2?family=Nunito:wght@300;400;600;700&display=swap" rel="stylesheet">
    <!-- Add this for better Avenir Next support if you have the license -->
    <link rel="stylesheet" href="https://use.typekit.net/your-project-id.css">
    <!-- Custom CSS -->
    <link href="styles.css" rel="stylesheet">
    <!-- VSCode Syntax Highlighting -->
    <link href="vscode-light-style.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/languages/python.min.js"></script>
</head>
<body>
    <!-- Header Section -->
    <header class="header-section" id="overview">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 mx-auto text-center">
                    <h1 class="paper-title">Transformers without Normalization</h1>
                    <div class="author-line text-center mb-2">
                        <span><a href="https://jiachenzhu.github.io/" class="text-light author-link">Jiachen&nbsp;Zhu<sup>1,2</sup></a></span>
                        <span class="mx-2 text-light">•</span>
                        <span><a href="https://xinleic.xyz/" class="text-light author-link">Xinlei&nbsp;Chen<sup>1</sup></a></span>
                        <span class="mx-2 text-light">•</span>
                        <span><a href="https://people.csail.mit.edu/kaiming/" class="text-light author-link">Kaiming&nbsp;He<sup>3</sup></a></span>
                        <span class="mx-2 text-light">•</span>
                        <span><a href="http://yann.lecun.com" class="text-light author-link">Yann&nbsp;LeCun<sup>1,2</sup></a></span>
                        <span class="mx-2 text-light">•</span>
                        <span><a href="https://liuzhuang13.github.io" class="text-light author-link">Zhuang&nbsp;Liu<sup>1,4,†</sup></a></span>
                    </div>
                    <div class="project-lead-line mt-2 mb-3 text-center">
                        <span class="text-light"><sup>†</sup> Project Lead</span>
                    </div>
                    <!-- Institution Logos Section (Replace the text-based institution line) -->
<!-- Institution Logos Section -->
<div class="institution-logos text-center mb-3">
    <span class="institution-logo-wrapper">
        <img src="webpage_assets/logos/meta.png" alt="FAIR, Meta" class="institution-logo" title="FAIR, Meta">
        <sup>1</sup>
    </span>
    <span class="dot-separator">•</span>
    <span class="institution-logo-wrapper">
        <img src="webpage_assets/logos/nyu.png" alt="New York University" class="institution-logo" title="New York University">
        <sup>2</sup>
    </span>
    <span class="dot-separator">•</span>
    <span class="institution-logo-wrapper mit-logo">
        <img src="webpage_assets/logos/mit.png" alt="MIT" class="institution-logo" title="MIT">
        <sup>3</sup>
    </span>
    <span class="dot-separator">•</span>
    <span class="institution-logo-wrapper princeton-logo">
        <img src="webpage_assets/logos/princeton.png" alt="Princeton University" class="institution-logo" title="Princeton University">
        <sup>4</sup>
    </span>
</div>

                <div class="mt-2 mb-3 text-center">
                    <span class="text-light">CVPR 2025</span>
                </div>

                    <div class="mt-4">
                        <a href="https://arxiv.org/abs/2503.10622" class="btn btn-light me-2" style="box-shadow: 0 4px 8px rgba(0,0,0,0.1);"><i class="fas fa-file-alt me-2" style="color: var(--primary-color);"></i>Paper</a>
                        <a href="https://github.com/jiachenzhu/DyT" class="btn btn-light me-2" style="box-shadow: 0 4px 8px rgba(0,0,0,0.1);"><i class="fab fa-github me-2" style="color: var(--primary-color);"></i>Code</a>
                        <a href="#" class="btn btn-light me-2" style="box-shadow: 0 4px 8px rgba(0,0,0,0.1);"><i class="fab fa-x-twitter me-2" style="color: var(--primary-color);"></i>Summary</a>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <main class="container">
        <!-- Figure Section -->
        <section id="figure" class="figure-section">
            <div class="seamless-figure">
                <img src="webpage_assets/before_after.svg" alt="Dynamic Tanh (DyT) as a replacement for normalization in Transformers" class="img-fluid">
                <div class="figure-caption">
                    <em>Left:</em> original Transformer block. <em>Right:</em> block with our proposed Dynamic Tanh (DyT) layer. <br />
                    DyT is a straightforward replacement for commonly used Layer Norm or RMSNorm layers. <br />
                    Transformers with DyT match or exceed the performance of their normalized counterparts.
                </div>
            </div>
        </section>

        <!-- Abstract Section -->
        <section id="abstract">
            <h2 class="section-title">Abstract</h2>
                <p>
                    Normalization layers are ubiquitous in modern neural networks and have long been considered essential.
                    This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique.
                    We introduce Dynamic Tanh (DyT), an element-wise operation $$\mathrm{DyT}(\boldsymbol{x}) = \tanh(\alpha \boldsymbol{x}),$$ as a drop-in replacement for normalization layers in Transformers.
                    DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, S-shaped input-output mappings.
                    By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning.
                    We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models.
                    These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks.
                </p>
        </section>
        
        <!-- Implementation Section -->
        <section id="implementation" class="implementation-section">
            <h2 class="section-title">Implementation</h2>
            <p>
                DyT module can be implemented in a few lines of PyTorch code:
            </p>
            
            <div class="code-container position-relative" style="margin: 20px 0;">
                <button id="copy-btn" class="btn btn-sm" onclick="copyCode()" style="position: absolute; top: 10px; right: 10px; z-index: 10;">
                    <i class="fas fa-copy"></i> Copy
                </button>
                <pre id="code-block-dyt" style="padding: 20px; padding-top: 50px; margin: 0; overflow-x: auto;"><code class="python">class DyT(nn.Module):
    def __init__(self, num_features, alpha_init_value=0.5):
        super().__init__()
        self.alpha = nn.Parameter(torch.ones(1) * alpha_init_value)
        self.weight = nn.Parameter(torch.ones(num_features))
        self.bias = nn.Parameter(torch.zeros(num_features))
    
    def forward(self, x):
        x = torch.tanh(self.alpha * x)
        return x * self.weight + self.bias</code></pre>
            </div>
        </section>

        <!-- Key Findings Section -->
        <section id="key-findings">
            <h2 class="section-title">Key Findings</h2>
            
            <div class="row mb-5">
                <div class="col-lg-12">
                    
                            <h4>Layer Normalization Behaves Like Scaled Tanh Function</h4>
                            <p>Our analysis shows that layer normalization (LN) in Transformers generates input-output mappings that closely resemble scaled tanh functions. In the earlier layers, these mappings are mostly linear. However, in deeper layers, they take on distinct S-shaped curves characteristic of tanh functions.</p>
                            
                            <div class="row figure-container my-4">
                                <div class="col-12 mb-3">
                                    <img src="webpage_assets/figures/inout_vit.png" alt="Input-output relationships in ViT normalization layers" class="img-fluid">
                                </div>
                                <div class="col-12 mb-3">
                                    <img src="webpage_assets/figures/inout_w2v.png" alt="Input-output relationships in wav2vec 2.0 normalization layers" class="img-fluid">
                                </div>
                                <div class="col-12 mb-3">
                                    <img src="webpage_assets/figures/inout_dit.png" alt="Input-output relationships in DiT normalization layers" class="img-fluid">
                                </div>
                                <div class="figure-caption">
                                    <em>Output vs. input of selected LN layers in Vision Transformer (ViT), wav2vec 2.0 (a Transformer model for speech), and Diffusion Transformer (DiT). We plot the input/output values of four LN layers in each model. The S-shaped curves highly resemble that of a tanh function.</em>
                                </div>
                            
                            </div>
                        
                </div>
            </div>


        <!-- Evaluation Section -->
        <section id="evaluation">
            <h2 class="section-title">Evaluation</h2>
            <p>
                We present a comprehensive evaluation of DyT across a diverse range of architectures and tasks, highlighting its effectiveness and generalizability.  
                Our experiments cover supervised learning in vision (<b>ViT</b> and <b>ConvNeXt</b>), self-supervised learning in vision (<b>MAE</b> and <b>DINO</b>), diffusion models (<b>DiT</b>), large language models (<b>LLaMA</b>), self-supervised learning in speech (<b>wav2vec 2.0</b>), and DNA sequence modeling (<b>HyenaDNA</b> and <b>Caduceus</b>).  
                In every case, Transformers with DyT achieves similar or better performance than their normalized counterparts.  
                For detailed results and comparisons, please refer to our paper.    
            </p>
        </section>                    
                        

            

        <!-- Resources Section -->
        <section id="resources">
            <h2 class="section-title">Resources</h2>
            <div class="row">
                <div class="col-md-4 mb-4">
                    <div class="card resources-card">
                        <div class="card-body">
                            <h5 class="card-title"><i class="fas fa-file-alt me-2" style="color: var(--primary-color);"></i>Paper</h5>
                            <p class="card-text">Download our paper for all the details about our research.</p>
                            <a href="https://arxiv.org/abs/2503.10622" class="btn btn-sm primary-btn">Download Paper</a>
                        </div>
                    </div>
                </div>
                <div class="col-md-4 mb-4">
                    <div class="card resources-card">
                        <div class="card-body">
                            <h5 class="card-title"><i class="fab fa-github me-2" style="color: var(--primary-color);"></i>Code</h5>
                            <p class="card-text">Check out our repository for implementation details.</p>
                            <a href="https://github.com/jiachenzhu/DyT" class="btn btn-sm secondary-btn">View on GitHub</a>
                        </div>
                    </div>
                </div>
                <div class="col-md-4 mb-4">
                    <div class="card resources-card">
                        <div class="card-body">
                            <h5 class="card-title"><i class="fab fa-x-twitter me-2" style="color: var(--primary-color);"></i>Summary</h5>
                            <p class="card-text">Read a concise summary of our research findings on X.</p>
                            <a href="#" class="btn btn-sm accent-btn">View Summary</a>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Citations Section -->
        <!-- Citations Section -->
<!-- Citations Section -->
<section id="citations" class="citation-section">
    <h2 class="section-title">BibTeX</h2>
            <div class="code-container position-relative" style="margin: 20px 0;">
                <button id="copy-citation-btn" class="btn btn-sm" onclick="copyCitation()" style="position: absolute; top: 10px; right: 10px; z-index: 10;">
                    <i class="fas fa-copy"></i> Copy
                </button>
                <pre id="code-block-citation" style="padding: 20px; padding-top: 50px; margin: 0; overflow-x: auto;"><code class="plaintext">@inproceedings{Zhu2025DyT,
  title={Transformers without Normalization},
  author={Zhu, Jiachen and Chen, Xinlei and He, Kaiming and LeCun, Yann and Liu, Zhuang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2025}
}</code></pre>

    </div>
</section>

        <section>
            <h2 class="section-title">Correspondence</h2>
            <div class="container text-center">
                <p>jiachen [dot] zhu [at] nyu [dot] edu</p>
                <p>zhuangl [at] princeton [dot] edu</p>
                <!-- <p></p> -->
            </div>
        </section>
    </main>

    <!-- Bootstrap and JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.2/js/bootstrap.bundle.min.js"></script>
    
    <!-- MathJax for LaTeX rendering -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js" async></script>
    <!-- Custom JS -->
    <script src="scripts.js"></script>
</body>
</html>